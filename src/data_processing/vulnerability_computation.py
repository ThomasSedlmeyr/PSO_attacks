import concurrent.futures

import numba
import numpy as np
import pandas as pd
from tqdm import tqdm


# get one hot encoding
def onehot_encode(df, full_uniq_vals):
    new_cols = []
    for col in full_uniq_vals.keys():
        for uniq_val in full_uniq_vals[col]:
            new_cols.append((df[col] == uniq_val).rename(f'{col}_{uniq_val}'))

    return pd.concat(new_cols, axis='columns')


# get vulnerability of a batch of records
def get_vuln_batch(df_np, record_inds, k):
    vulns = np.zeros(len(df_np))
    for record_ind in tqdm(record_inds, leave=False):
        record = df_np[int(record_ind)]

        # calculate distance between record and other records
        distances = np.sqrt(np.sum(np.square(record - df_np), axis=1))
        distances.sort()

        # drop first distance because that will be the distance between the record and itself
        # calculate vulnerability score of record
        vuln = distances[1: k +1].mean()

        vulns[record_ind] = vuln
    return vulns

# get vulnerability of single record
def get_vuln_single(df_np, record_index, k):
    record = df_np[record_index]

    # calculate distance between record and other records
    distances = np.sqrt(np.sum(np.square(record - df_np), axis=1))
    distances.sort()

    # drop first distance because that will be the distance between the record and itself
    # calculate vulnerability score of record
    vuln = distances[1: k +1].mean()
    return vuln


# get vulnerability score of records
def get_vuln_single_threaded(df, full_uniq_vals, k=5, show_progress=False):
    vulns = np.zeros(len(df))

    # convert to onehot encoding
    df_np = onehot_encode(df, full_uniq_vals)
    df_np = df_np.astype(int).to_numpy()

    pbar = tqdm(range(len(df)), desc="Computing vulnerability") if show_progress else range(len(df))
    for i in pbar:
        vulns[i] = get_vuln_single(df_np, i, k)
    return vulns


def get_vuln_single_wrapper(args, show_progress=False):
    df_np, indices, k = args
    if show_progress:
        return [get_vuln_single(df_np, i, k) for i in tqdm(indices, desc="Processing Chunk", mininterval=1.0)]
    else:
        return [get_vuln_single(df_np, i, k) for i in indices]

def split_in_equal_parts(list_to_split, n_parts):
    k, m = divmod(len(list_to_split), n_parts)
    return (list_to_split[ i * k +min(i, m):( i +1 ) * k +min( i +1, m)] for i in range(n_parts))


def get_vuln_multi_threaded(df, full_uniq_vals, k=5, show_progress=False, max_workers=None):
    vulns = np.zeros(len(df))
    df_np = onehot_encode(df, full_uniq_vals)
    df_np = df_np.astype(int).to_numpy()

    num_chunks = max_workers or concurrent.futures.ProcessPoolExecutor(max_workers)._max_workers
    chunks = split_in_equal_parts(list(range(len(df_np))), num_chunks)
    args = [(df_np, chunk, k) for chunk in chunks]

    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for idx, arg in enumerate(args):
            futures.append(executor.submit(get_vuln_single_wrapper, arg, show_progress))

        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Overall Progress"):
            pass

    results = [f.result() for f in futures]
    results = [item for sublist in results for item in sublist]
    vulns[:] = results
    return vulns

def compute_vulnerability(df_cat, target_path=None, multi_processing=True, number_threads=None, show_progress=True):
    full_uniq_vals = {}
    for col in df_cat.columns:
        full_uniq_vals[col] = df_cat[col].unique().tolist()

    if multi_processing:
        vulns = get_vuln_multi_threaded(df_cat, full_uniq_vals, show_progress=show_progress, max_workers=number_threads)
    else:
        vulns = get_vuln_single_threaded(df_cat, full_uniq_vals, show_progress=show_progress)

    if target_path:
        np.savetxt(target_path + 'vulns.txt', vulns)
    return vulns


@numba.njit(parallel=True)
def compute_euclidian_distance_matrix(df_np):
    n = df_np.shape[0]
    result = np.empty((n, n))
    for i in numba.prange(n):
        for j in range(n):
            sum_sq = 0.0
            for k in range(df_np.shape[1]):
                diff = df_np[i, k] - df_np[j, k]
                sum_sq += diff * diff
            result[i, j] = np.sqrt(sum_sq)
    return result


@numba.njit(parallel=True)
def compute_hamming_distance_matrix(df_np):
    n = df_np.shape[0]
    result = np.empty((n, n))
    for i in numba.prange(n):
        for j in range(n):
            result[i, j] = np.sum(df_np[i] != df_np[j])
    return result


@numba.njit(parallel=True)
def compute_cosine_similarity_distance_matrix(df_np):
    n = df_np.shape[0]
    result = np.empty((n, n))
    for i in numba.prange(n):
        for j in range(n):
            result[i, j] = 1 - np.dot(df_np[i], df_np[j]) / (np.linalg.norm(df_np[i]) * np.linalg.norm(df_np[j]))
    return result


def compute_distance_matrix_2(df_np):
    result = np.empty((len(df_np), len(df_np)))
    for i in range(len(df_np)):
        result[i, :] = np.sqrt(np.sum(np.square(df_np[i] - df_np[:]), axis=1))
    return result

@numba.njit(parallel=True)
def sort_distances_and_compute_vuln(result, k):
    n = result.shape[0]
    sorted_result = np.empty_like(result)
    for i in numba.prange(n):
        sorted_result[i] = np.sort(result[i])
    vuln = np.empty(n)
    for i in numba.prange(n):
        vuln[i] = np.mean(sorted_result[i, 1:k + 1])  # Exclude the zero distance to itself at index 0
    return vuln


def compute_distances_fast(data: pd.DataFrame, k=5, metric='euclidean', num_threads=None):
    # Precompute unique values and encode
    if num_threads:
        numba.set_num_threads(num_threads)
    full_uniq_vals = {col: data[col].unique().tolist() for col in data.columns}
    df_np = onehot_encode(data, full_uniq_vals).astype(int).to_numpy()

    # Compute the full distance matrix
    if metric == 'euclidean':
        distance_matrix = compute_euclidian_distance_matrix(df_np)
    elif metric == 'hamming':
        distance_matrix = compute_hamming_distance_matrix(df_np)
    elif metric == 'cosine':
        distance_matrix = compute_cosine_similarity_distance_matrix(df_np)
    else:
        raise ValueError(f"Unknown metric: {metric}")
    print("computed distances")

    # Find k nearest distances for each point
    #vuln = sort_distances_and_compute_vuln(distance_matrix, k)
    distance_matrix.sort()
    # drop first distance because that will be the distance between the record and itself
    # calculate vulnerability score of record
    vuln = distance_matrix[1: k + 1].mean()
    return vuln

